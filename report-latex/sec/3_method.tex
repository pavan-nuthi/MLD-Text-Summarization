\section{Methodology}
\label{sec:method}

\begin{figure}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{architecture.png}
   \caption{Overview of the text summarization architectures. Left: BERTSum extractive model. Right: BART abstractive model.}
   \label{fig:architecture}
\end{figure}

In this section, we describe the two architectures employed in our study. Figure \ref{fig:architecture} provides a high-level overview of both the BERTSum and BART models, while Figure \ref{fig:network_arch} details the specific network components.

\begin{figure*}[t]
  \centering
   \includegraphics[width=1.0\linewidth]{network_architecture.png}
   \caption{Detailed network architecture. Left: BERTSum with inter-sentence Transformer. Right: BART encoder-decoder with cross-attention.}
   \label{fig:network_arch}
\end{figure*}

\subsection{BERTSum (Extractive)}
For our extractive baseline, we employ BERTSum \cite{liu2019text}. Standard BERT is trained as a masked language model and next-sentence predictor, which is not directly optimized for sentence selection. BERTSum adapts BERT by inserting a \texttt{[CLS]} token at the beginning of each sentence in the input document. The vector representation of these \texttt{[CLS]} tokens is then passed through a summarization layer (a simple linear classifier) to predict a binary label $y_i \in \{0, 1\}$ for each sentence, indicating whether it should be included in the summary.

We fine-tune the model using the binary cross-entropy loss against "oracle" labels, which are generated by greedily selecting sentences from the document that maximize the ROUGE score with respect to the reference summary.

\subsection{BART (Abstractive)}
For our abstractive model, we use BART (Bidirectional and Auto-Regressive Transformers) \cite{lewis2019bart}. BART combines a bidirectional encoder (like BERT) with an auto-regressive decoder (like GPT). This architecture makes it particularly suitable for sequence-to-sequence tasks where the input is a noisy text and the output is a clean version (or summary).

We fine-tune the \texttt{bart-base} model using the standard maximum likelihood estimation (MLE) loss. The model takes the full source document as input and generates the summary token by token. During inference, we use beam search to generate the final summary sequence.
